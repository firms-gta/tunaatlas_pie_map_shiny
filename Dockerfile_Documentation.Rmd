# Dockerfile Documentation

## Introduction

This document provides a detailed explanation of the Dockerfile used to set up and deploy a Shiny application. The Dockerfile includes steps for configuring the environment, managing dependencies, and running the Shiny app. This Dockerfile can be used either on its own or in conjunction with the .yml file designed for the CI/CD pipeline on GitHub.

This Documentation is based on this Dockerfile: https://github.com/firms-gta/tunaatlas_pie_map_shiny/blob/main/Dockerfile, if you want to reproduce something similar, make sure to update the r-ver the label maintaner and the working directory.

---

```dockerfile
FROM rocker/r-ver:4.2.3
```

- **Base Image**: Uses the `rocker/r-ver` base image with R version 4.2.3 as the foundation for the container. (use the version of R you've been deploying the app on)

---

```dockerfile
LABEL maintainer="Julien Barde <julien.barde@ird.fr>"
```

- **Maintainer Information**: Adds metadata about the person maintaining this Dockerfile.

---

```dockerfile
RUN apt-get update && apt-get install -y \
    sudo \
    pandoc \
    pandoc-citeproc \
    libssl-dev \
    libcurl4-gnutls-dev \
    libxml2-dev \
    libudunits2-dev \
    libproj-dev \
    libgeos-dev \
    libgdal-dev \
    libv8-dev \
    libsodium-dev \
    libsecret-1-dev \
    git \
    libnetcdf-dev \
    curl \
    libjq-dev \
    cmake \
    protobuf-compiler \
    libprotobuf-dev \
    librdf0 \
    librdf0-dev \
    redland-utils && \
    apt-get clean
```

- **System Dependencies**:
  - Installs system libraries required for building and running the Shiny app.
  - Includes dependencies like `pandoc` for document conversion, `libssl-dev` for SSL/TLS, and others for geospatial and network operations.
If you want lighter app you will have to reduce the system libraries used, this can be done by removing some and testing the app. If you want to do that I recommend doing it in local rather than with the github CI/CD pipeline.

---

```dockerfile
RUN install2.r --error --skipinstalled --ncpus -1 httpuv
```

- Installs the `httpuv` R package, which is a core dependency for Shiny apps.

---

```dockerfile
WORKDIR /root/tunaatlas_pie_map_shiny
```

- **Set Working Directory**: Specifies the working directory inside the container.

---

```dockerfile
COPY download_GTA_data.R ./download_GTA_data.R
RUN Rscript download_GTA_data.R
```

- **Data Preparation**:
  - Copies a script into the container to download necessary datasets, from .zip or .csv files on github.
  - Executes the script using `Rscript`.

This step concern the downloading of data from zip on github. As the script is the less likely to change, this step is at the beggining of the Dockerfile, thus, it will be cached when building.

---

```dockerfile
RUN R -e "install.packages('remotes', repos='https://cran.r-project.org/')" 
RUN R -e "remotes::install_version('downloader', version = '0.4', upgrade = 'never', repos = 'https://cran.r-project.org/')"
RUN R -e "remotes::install_version('readr', version = '2.1.5', upgrade = 'never',  repos = 'https://cran.r-project.org/')"

```

- **Downloading packages for downloading big data**:
  - Download packages to download data from DOI (around 300 MB).
  - This can be done using other packages, it can also be integrated to a script.
  
This step is also one of the less likely to change, and the less 

---

```dockerfile
## Echo the DOI_CSV_HASH for debugging and to to stop cache if DOI.csv has changed (takes in input the hash of the DOI.csv file created in yml)
ARG DOI_CSV_HASH
RUN echo "DOI_CSV_HASH=${DOI_CSV_HASH}" > /tmp/doi_csv_hash.txt

## Create data repository to copy DOI.csv, a file listing the dataset to download from zenodo
RUN mkdir -p data 

## Copy the CSV containing the data to download
## Copy the script downloading the data from the CSV
COPY DOI.csv ./DOI.csv 
COPY update_data.R ./update_data.R 
RUN Rscript update_data.R 
```

This download data from zenodo taking in input a .csv file providing every DOI of the dataset to be downloaded. As this operation takes time and bandwidth, it is strongly recommended to use the cache. For this we check the DOI.csv file and if it didn't change this operation is not reran. To strongly invalidate or validate the cache we choose to save the hash in a local file as just echoing showed some limitations.

NB: It is not yet clear if the renv.lock is changing less than the DOI.csv in the developpement of our shiny app, both of this steps are bandwith and time consuming. Once the DOI.csv is set and is supposed not to change it is passed before the renv.lock hashing.

```dockerfile
## ARG defines a constructor argument called RENV_PATHS_ROOT. Its value is passed from the YAML file. An initial value is set up in case the YAML does not provide one
ARG RENV_PATHS_ROOT=/root/.cache/R/renv
ENV RENV_PATHS_ROOT=${RENV_PATHS_ROOT}

## Set environment variables for renv cache
ENV RENV_PATHS_CACHE=${RENV_PATHS_ROOT}

## Echo the RENV_PATHS_ROOT for logging
RUN echo "RENV_PATHS_ROOT=${RENV_PATHS_ROOT}"
RUN echo "RENV_PATHS_CACHE=${RENV_PATHS_CACHE}"

## Define the build argument for the hash of renv.lock to stop cache if renv.lock has changed
ARG RENV_LOCK_HASH
RUN if [ -z "${RENV_LOCK_HASH}" ]; then \
      export RENV_LOCK_HASH=$(sha256sum renv.lock | cut -d' ' -f1); \
    fi && \
    echo "RENV_LOCK_HASH=${RENV_LOCK_HASH}" > /tmp/renv_lock_hash.txt

## Create the renv cache directory
RUN mkdir -p ${RENV_PATHS_ROOT}

## Install renv package that records the packages used in the shiny app
RUN R -e "install.packages('renv', repos='https://cran.r-project.org/')"

## Copy renv configuration and lockfile
COPY renv.lock ./
COPY renv/activate.R renv/
COPY renv/settings.json renv/

## Restore renv packages
RUN R -e "renv::activate()" 
## Used to setup the environment (with the path cache)
RUN R -e "renv::restore()" 

## Copy the rest of the application code
COPY . .
```

Similarly, if renv.lock as not changed, all the downloading of the packages will not be done again as the cache image already contains them. We then activate the project to use renv and thus to load the corresonding packages. This action can be put at the all beginning of the dockerfile if the packages are the input that are the less likely to change. NB: Any change in the Dockerfile will remove the use of the cache for the next creation. 
The ARG RENV_LOCK_HASH corresponds in the yml to $(sha256sum renv.lock | cut -d' ' -f1). 

```dockerfile
## Create the default dataset from DOI and GTA data loading to make launching faster (use of qs for loading and data.table for tidying) 
RUN Rscript ./create_or_load_default_dataset.R 
```

Those lines are very specific to this shiny app, they allow the loading of the dataset and the tidying before launching the app and thus reduce the launching time. They now include as well the removing of some not-usefull-anymore dataset to reduce the size of the Dockerimage.

```dockerfile

## Expose port 3838 for the Shiny app
EXPOSE 3838

## Create directories for configuration
RUN mkdir -p /etc/tunaatlas_pie_map_shiny/

RUN R -e "library(sf); library(tmap); library(dplyr); library(ggplot2); library(leaflet); library(data.table)"
```

Exposure and directories are mandatory for the launching.

```dockerfile
RUN R -e "library(sf); library(tmap); library(dplyr); library(ggplot2); library(leaflet); library(data.table)"
```

Those lines are to load big packages prior of the launching to avoid time consuming however it is not clear yet if it works well. #Update it does not.

Some good practice can help reducing the time of the launching of the application, as reducing the packages used.

## Final Steps

At the end of the Dockerfile:
- The `CMD` statement ensures the Shiny app starts when the container runs:
```dockerfile
CMD ["R", "-e", "shiny::runApp('/root/tunaatlas_pie_map_shiny', port=3838, host='0.0.0.0')"]
```

- The port `3838` is exposed to allow access to the app.

---

